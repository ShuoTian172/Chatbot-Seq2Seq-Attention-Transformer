{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "xx0kY6zDtGGJ",
    "outputId": "bda404f6-3753-4fba-dd88-57e08d838286"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie_lines.txt', 'raw_script_urls.txt', 'movie_characters_metadata.txt', 'movie_conversations.txt', 'chameleons.pdf', 'README.txt', 'movie_titles_metadata.txt', '.DS_Store']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "8UGPno9UtGGQ"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('../input/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('../input/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "3983a4469fd5ed2f4b8db5a6cc41eda070c9142f",
    "colab": {},
    "colab_type": "code",
    "id": "bHfgrPBNtGGS"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "260fe85783607a5f8190796b4415dee00cdeabc5",
    "colab": {},
    "colab_type": "code",
    "id": "CI6Y_cPAtGGU"
   },
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c7ac8d38b75b1101165c37cdab3fca42597eb4d7",
    "colab": {},
    "colab_type": "code",
    "id": "uAsEvX1MtGGW",
    "outputId": "0561aa19-9015-4328-ca6d-7914e6318d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3490 That's what he did to me.  He put cigarettes out on me.\n",
      "L3491 Your father put cigarettes out on you?\n",
      "L3492 Out on my back when I was a small boy.\n",
      "L3493 Can I see your back?\n"
     ]
    }
   ],
   "source": [
    "#id and conversation sample\n",
    "for k in convs[300]:\n",
    "    print (k, id2line[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "a3142e705c67fd80837a998a7884dd584c30f57c",
    "colab": {},
    "colab_type": "code",
    "id": "Rb_cfF-9tGGZ",
    "outputId": "c1a31eb5-74fe-45b3-c9d4-7b066fa1510c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "85f3e1713f0620f066f5a5ee34b31c9a73d768fa",
    "colab": {},
    "colab_type": "code",
    "id": "WUQpBfpqtGGd"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "162b9f09f2e3e50ed92f157a5ce0faf3bb0d0019",
    "colab": {},
    "colab_type": "code",
    "id": "_B7ADOldtGGg"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "b1df72b880bbf79a8043ae3b608d37e89b5807e9",
    "colab": {},
    "colab_type": "code",
    "id": "lp5TWIq8tGGi",
    "outputId": "102388f8-e418-450f-aeb4-e47001cb2251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# Find the length of sentences (not using nltk due to processing speed)\n",
    "lengths = []\n",
    "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "0035c49ba3da36e903a600b22c737e6a5c34de6e",
    "colab": {},
    "colab_type": "code",
    "id": "NRE_xEDhtGGn",
    "outputId": "1d642d7d-8ab6-45d6-db7d-cd3e6e33780f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138528\n",
      "138528\n"
     ]
    }
   ],
   "source": [
    "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "for i, question in enumerate(clean_questions):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for i, answer in enumerate(short_answers_temp):\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "        \n",
    "print(len(short_questions))\n",
    "print(len(short_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "85389732fd564d7b160339bb26d0d1b5dea41fe7",
    "colab": {},
    "colab_type": "code",
    "id": "_Ve2HxzNtGGr",
    "outputId": "f3e8a9e1-1da1-4eef-e72c-26890bea556c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah, well, uh, you are not so smart, chief, 'cause i am moving out to l.a.\n",
      "ah, that is nice. they have many convenience stores there for you to stand in front of.\n",
      "\n",
      "hey, yes! hey, pony, man! great concert tonight!\n",
      "oh, you were there?\n",
      "\n",
      "oh, you were there?\n",
      "no, but i heard it was great.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = np.random.randint(1,len(short_questions))\n",
    "\n",
    "for i in range(r, r+3):\n",
    "    print(short_questions[i])\n",
    "    print(short_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5825a5967e5a04f13b0a755e8ba304c419af4b33",
    "colab_type": "text",
    "id": "qajlIsvGtGGv"
   },
   "source": [
    "### 1.1  Preprocessing for word based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "106248b9b0ac7499c2b34cc3ae1c20a2e0ea41c4",
    "colab": {},
    "colab_type": "code",
    "id": "xuBiwRxQtGGw"
   },
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 30000  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "8b37316d3b68626ed388bbb73863364608027796",
    "colab": {},
    "colab_type": "code",
    "id": "7rn6XeCFtGGy",
    "outputId": "208d162b-3cb8-4549-ba44-262dfd8e6742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 24000\n",
      "validation size 6000\n"
     ]
    }
   ],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3aacc0315837faf77599da087f8b3a52d217859",
    "colab_type": "text",
    "id": "gGOwO1p7tGG1"
   },
   "source": [
    "### 1.2  Word en/decoding dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "51918567f4f43ae13a8d58a00bda15b1d508b191",
    "colab": {},
    "colab_type": "code",
    "id": "J26jm7yHtGG1"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "# Create \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "30d25056e8f62453bcde0a16c8a7933275f431c9",
    "colab": {},
    "colab_type": "code",
    "id": "07YrRRsxtGG5"
   },
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 15\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4c980b035173d12d2bf06dafd6410eb5da6023b2",
    "colab": {},
    "colab_type": "code",
    "id": "Oy348hUctGG9",
    "outputId": "1306ad29-f1d8-410c-959a-a3503371888e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 16560\n",
      "Size of vocab we will use: 1938\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d62a629773e34bb7bc1a12508cce820c0e47962d",
    "colab": {},
    "colab_type": "code",
    "id": "SJCoR8FQtGHB",
    "outputId": "408c8cad-1b15-49b3-e468-adfb83958524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocab used: 1940\n"
     ]
    }
   ],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "de2e46d3826def6b6c94a9827c32118d555fae2e",
    "colab": {},
    "colab_type": "code",
    "id": "ZOKWOEJ8tGHF"
   },
   "outputs": [],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "bccd51e8730e7c3da1234af9480da767f400c230",
    "colab": {},
    "colab_type": "code",
    "id": "qU77ML3utGHI",
    "outputId": "05db4ba1-8d50-4d59-bede-56db914e73b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = word_num+1\n",
    "dict_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ea3c32a6180ee8fea8b1490b40351897cc426e6",
    "colab_type": "text",
    "id": "jkASDwkttGHN"
   },
   "source": [
    "### 1.3  Vectorizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4de33f91476bd6c50cf7c9322f273079783ceec2",
    "colab": {},
    "colab_type": "code",
    "id": "8MYqMrjxtGHO"
   },
   "outputs": [],
   "source": [
    "def transform(encoding, data, vector_size=20):\n",
    "   \n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "f13007fbe8661c4c2ea4c35fd51b86c4fc220efb",
    "colab": {},
    "colab_type": "code",
    "id": "9GEoMSottGHP",
    "outputId": "8588505a-d236-4ef5-ad7e-3efd1a31a77a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (24000, 20)\n",
      "encoded_training_output (24000, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding training set\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "cbbf370a5d0640a4dea6b6d9237a5917978b92a8",
    "colab": {},
    "colab_type": "code",
    "id": "CMVJzvSGtGHV",
    "outputId": "a1b0b360-ed24-4f0b-882b-4ebd41332786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_validation_input (6000, 20)\n",
      "encoded_validation_output (6000, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding validation set\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b52f0368fecf8ac79903927799fe67c104d8e518",
    "colab_type": "text",
    "id": "knUVplA9tGHe"
   },
   "source": [
    "## 2  Model Building\n",
    "### 2.1  Sequence-to-Sequence in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "e8b5afc77468e4e263b3fc7ff62a2e6952a80e97",
    "colab": {},
    "colab_type": "code",
    "id": "W75N-aGPtGHe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "306b1fb012febd74dd40840c0588bdd8503ba65b",
    "colab": {},
    "colab_type": "code",
    "id": "t72iDkrytGHi"
   },
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d91367ec6c706059c4b520011fdec6e1051db38d",
    "colab": {},
    "colab_type": "code",
    "id": "Cxm6EnP-tGHu",
    "outputId": "cd317479-08e0-4ae0-bfa5-0703988cd5d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder Tensor(\"lstm_3/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
      "encoder_last Tensor(\"strided_slice_1:0\", shape=(?, 512), dtype=float32)\n",
      "decoder Tensor(\"lstm_4/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)\n",
    "\n",
    "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "print('decoder', decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2feb58fa60a8ff335b218114808aa2d3e4d37461",
    "colab_type": "text",
    "id": "MR8exZgktGHy"
   },
   "source": [
    "### 2.2  Attention Mechanism\n",
    "Reference: Effective Approaches to Attention-based Neural Machine Translation's Global Attention with Dot-based scoring function (Section 3, 3.1) https://arxiv.org/pdf/1508.04025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIuj9jSztGHz",
    "outputId": "31ec9b17-49cb-4aae-b27b-dca18ab18748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
      "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
      "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
      "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 1941), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('attention', attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "print('context', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "257dcf12770f8928f91754044d3efc5aff1ec296",
    "colab": {},
    "colab_type": "code",
    "id": "CPKzF7r9tGH1",
    "outputId": "892241f5-8eaf-49b8-bc8c-26a6eb0f7783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 20, 128)      248448      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 128)      248448      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 20, 512)      1312768     embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 20, 512)      1312768     embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           lstm_4[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 1941)     995733      time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 4,642,965\n",
      "Trainable params: 4,642,965\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "09b61276a10c14838eb09db124a229ac58c61133",
    "colab": {},
    "colab_type": "code",
    "id": "PjhKvOJhtGH3"
   },
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output =np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START\n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "3d43e174878dfbef64900b7f043d7bb41f458309",
    "colab": {},
    "colab_type": "code",
    "id": "d3ttEzAvtGH6",
    "outputId": "e30e3010-d012-4248-f059-5c08bb99fd1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 2/50\n",
      "24000/24000 [==============================] - 23s 977us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 3/50\n",
      "24000/24000 [==============================] - 24s 993us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 4/50\n",
      "24000/24000 [==============================] - 23s 973us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 5/50\n",
      "24000/24000 [==============================] - 24s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 6/50\n",
      "24000/24000 [==============================] - 24s 984us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 7/50\n",
      "24000/24000 [==============================] - 24s 985us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 8/50\n",
      "24000/24000 [==============================] - 23s 971us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 9/50\n",
      "24000/24000 [==============================] - 24s 987us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 10/50\n",
      "24000/24000 [==============================] - 24s 986us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 11/50\n",
      "24000/24000 [==============================] - 24s 986us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 12/50\n",
      "24000/24000 [==============================] - 23s 978us/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 13/50\n",
      "24000/24000 [==============================] - 24s 980us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 14/50\n",
      "24000/24000 [==============================] - 23s 972us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 15/50\n",
      "24000/24000 [==============================] - 23s 969us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 16/50\n",
      "24000/24000 [==============================] - 23s 975us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 17/50\n",
      "24000/24000 [==============================] - 23s 970us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 18/50\n",
      "24000/24000 [==============================] - 24s 987us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 19/50\n",
      "24000/24000 [==============================] - 23s 967us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 20/50\n",
      "24000/24000 [==============================] - 23s 967us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 21/50\n",
      "24000/24000 [==============================] - 23s 963us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 22/50\n",
      "24000/24000 [==============================] - 23s 976us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 23/50\n",
      "24000/24000 [==============================] - 23s 975us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 24/50\n",
      "24000/24000 [==============================] - 23s 978us/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 25/50\n",
      "24000/24000 [==============================] - 23s 962us/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 26/50\n",
      "24000/24000 [==============================] - 23s 967us/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 27/50\n",
      "24000/24000 [==============================] - 23s 978us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 28/50\n",
      "24000/24000 [==============================] - 23s 978us/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 29/50\n",
      "24000/24000 [==============================] - 23s 977us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 30/50\n",
      "24000/24000 [==============================] - 24s 980us/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 31/50\n",
      "24000/24000 [==============================] - 24s 997us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 32/50\n",
      "24000/24000 [==============================] - 23s 976us/step - loss: 0.0011 - val_loss: 0.0030\n",
      "Epoch 33/50\n",
      "24000/24000 [==============================] - 24s 980us/step - loss: 0.0010 - val_loss: 0.0031\n",
      "Epoch 34/50\n",
      "24000/24000 [==============================] - 23s 975us/step - loss: 9.8190e-04 - val_loss: 0.0032\n",
      "Epoch 35/50\n",
      "24000/24000 [==============================] - 23s 977us/step - loss: 9.3035e-04 - val_loss: 0.0032\n",
      "Epoch 36/50\n",
      "24000/24000 [==============================] - 23s 971us/step - loss: 8.8044e-04 - val_loss: 0.0033\n",
      "Epoch 37/50\n",
      "24000/24000 [==============================] - 24s 979us/step - loss: 8.3134e-04 - val_loss: 0.0034\n",
      "Epoch 38/50\n",
      "24000/24000 [==============================] - 23s 960us/step - loss: 7.8499e-04 - val_loss: 0.0035\n",
      "Epoch 39/50\n",
      "24000/24000 [==============================] - 23s 957us/step - loss: 7.4511e-04 - val_loss: 0.0035\n",
      "Epoch 40/50\n",
      "24000/24000 [==============================] - 23s 960us/step - loss: 7.0256e-04 - val_loss: 0.0036\n",
      "Epoch 41/50\n",
      "24000/24000 [==============================] - 23s 966us/step - loss: 6.6057e-04 - val_loss: 0.0037\n",
      "Epoch 42/50\n",
      "24000/24000 [==============================] - 23s 964us/step - loss: 6.2030e-04 - val_loss: 0.0037\n",
      "Epoch 43/50\n",
      "24000/24000 [==============================] - 23s 965us/step - loss: 5.8130e-04 - val_loss: 0.0038\n",
      "Epoch 44/50\n",
      "24000/24000 [==============================] - 24s 979us/step - loss: 5.4449e-04 - val_loss: 0.0039\n",
      "Epoch 45/50\n",
      "24000/24000 [==============================] - 23s 967us/step - loss: 5.0660e-04 - val_loss: 0.0039\n",
      "Epoch 46/50\n",
      "24000/24000 [==============================] - 23s 968us/step - loss: 4.6952e-04 - val_loss: 0.0040\n",
      "Epoch 47/50\n",
      "24000/24000 [==============================] - 23s 955us/step - loss: 4.3366e-04 - val_loss: 0.0041\n",
      "Epoch 48/50\n",
      "24000/24000 [==============================] - 23s 958us/step - loss: 4.0076e-04 - val_loss: 0.0041\n",
      "Epoch 49/50\n",
      "24000/24000 [==============================] - 23s 958us/step - loss: 3.8246e-04 - val_loss: 0.0042\n",
      "Epoch 50/50\n",
      "24000/24000 [==============================] - 23s 962us/step - loss: 3.5440e-04 - val_loss: 0.0042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa6bc85c88>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          #validation_split=0.05,\n",
    "          batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "874f0398f10f48cce993bcd9aa7b5e0644b6f292",
    "colab_type": "text",
    "collapsed": true,
    "id": "b12stZYitGH9"
   },
   "source": [
    "## 3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d0e3217f3d250c253dd457f192f54f8cd67c630f",
    "colab": {},
    "colab_type": "code",
    "id": "O1Qs6-DptGH-"
   },
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    clean_input = clean_text(raw_input)\n",
    "    input_tok = [nltk.word_tokenize(clean_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output\n",
    "\n",
    "def decode(decoding, vector):\n",
    "    \n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "16e6ece57e4c02321c58176e5471f37ad1170837",
    "colab": {},
    "colab_type": "code",
    "id": "FpjJQlS5tGH_",
    "outputId": "0002d6b7-701b-4070-8195-46e922c0e2dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: you like it?\n",
      "A:  i do not know ... i just got to do to give me that myself .\n",
      "Q: you really should not eat like that. all that sugar. it is not good for you.\n",
      "A:  well , you got a lot of <UNK> , for god 's sake , is not it ?\n",
      "Q: that is that?\n",
      "A:  you <UNK> seen him and shot you went killed him ... ? out there she is still a <UNK> <UNK>\n",
      "Q: i cannot do it, louis. it is not ethical. i could lose my license.\n",
      "A:  why can not you just have them <UNK> ? you are a doctor .\n",
      "Q: maude, do you pray?\n",
      "A:  i am really sorry .\n",
      "Q: roberto, what the fuck?\n",
      "A:  the <UNK> , just <UNK> you as your lord , you know that ?\n",
      "Q: well i am\n",
      "A:  i know a man who needs a <UNK> .\n",
      "Q: shall we let the people come in?\n",
      "A:  of course , let them in ! you are late now .\n",
      "Q: you are in a hurry.\n",
      "A:  yeah , i been waiting three years .\n",
      "Q: i beg your pardon?\n",
      "A:  <UNK> , i am number one . i went to see her a room and <UNK> the other stuff stuff\n",
      "Q: he killed his father and then my family...\n",
      "A:  <UNK> , i have your family . they really say anything . we are having two <UNK> of your your\n",
      "Q: goodness, no!\n",
      "A:  now that is true ? you could have a chance of <UNK> as a <UNK> by which ...\n",
      "Q: that i be brought here? who had that brilliant idea?\n",
      "A:  why do you want to see it ?\n",
      "Q: what could be worse than disappoint ing a little girl?\n",
      "A:  <UNK> a <UNK> girl !\n",
      "Q: how is the ipo?\n",
      "A:  <UNK> <UNK> <UNK> , <UNK> <UNK> .\n",
      "Q: god, she is going to kill me... this bottle is halfempty!\n",
      "A:  that is great ! <UNK> , my dad was never <UNK> it on other , my dear , <UNK> <UNK>\n",
      "Q: spunky. he told you about that?\n",
      "A:  all he could think about was getting to you . there 's still got a guy .\n",
      "Q: i would be happy to mention it to him.\n",
      "A:  be better not for <UNK> .\n",
      "Q: he wants to apologise.\n",
      "A:  he should be <UNK> and <UNK> . but not if he is <UNK> mantan .\n",
      "Q: no. she was an immigrant.\n",
      "A:  that is an <UNK> ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-6UwBDvtGIH"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyvBrBlktGIJ"
   },
   "outputs": [],
   "source": [
    "X = short_questions\n",
    "Y = short_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6y5V9_KptGIK",
    "outputId": "a77cdeab-38fe-49d6-9a9f-5d3bc74fba3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "len (Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wpngf-4xtGIN",
    "outputId": "9bd330e5-a3aa-41fd-d009-703d4fe81ea6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [22:14<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "fr_preds = []\n",
    "for sentence in tqdm.tqdm(X_test):\n",
    "    fr_pred = decode(decoding, prediction(sentence)[0])\n",
    "    fr_preds.append(fr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0Td14V2tGIP"
   },
   "outputs": [],
   "source": [
    "references = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OOmLBqltGIR",
    "outputId": "8c7ba139-b45e-4b6c-a7e4-9a1a465199d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 6000/6000 [00:03<00:00, 1739.24it/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_score_1 = []\n",
    "bleu_score_2 = []\n",
    "bleu_score_3 = []\n",
    "bleu_score_4 = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(fr_preds))):\n",
    "   \n",
    "    pred = fr_preds[i].replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").replace(\"<UNK>\", \"\").replace(\"<GO>\", \"\").rstrip()\n",
    "    reference = references[i].lower().replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").replace(\"<UNK>\", \"\").replace(\"<GO>\", \"\").rstrip()\n",
    "\n",
    "    score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
    "    score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0, 1, 0, 0))\n",
    "    score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 1, 0))\n",
    "    score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 0, 1))\n",
    "    \n",
    "    \n",
    "#     score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
    "#     score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0.5, 0.5, 0, 0))\n",
    "#     score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0.34, 0.33, 0.33, 0.))\n",
    "#     score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    \n",
    "    \n",
    "    bleu_score_1.append(score_1)\n",
    "    bleu_score_2.append(score_2)\n",
    "    bleu_score_3.append(score_3)\n",
    "    bleu_score_4.append(score_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhGZEkW2tGIe",
    "outputId": "e6fe1794-3f8a-4d0c-8c25-b65f9d37deb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score individual 1-gram on our corpus is about 0.25633715409574215\n",
      "The BLEU score individual 2-gram on our corpus is about 0.39669821445366915\n",
      "The BLEU score individual 3-gram on our corpus is about 0.44920929871055243\n",
      "The BLEU score individual 4-gram on our corpus is about 0.4996404942620233\n"
     ]
    }
   ],
   "source": [
    "print(\"The BLEU score individual 1-gram on our corpus is about {}\".format(sum(bleu_score_1) / len(bleu_score_1)))\n",
    "print(\"The BLEU score individual 2-gram on our corpus is about {}\".format(sum(bleu_score_2) / len(bleu_score_2)))\n",
    "print(\"The BLEU score individual 3-gram on our corpus is about {}\".format(sum(bleu_score_3) / len(bleu_score_3)))\n",
    "print(\"The BLEU score individual 4-gram on our corpus is about {}\".format(sum(bleu_score_4) / len(bleu_score_4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZ6eCH18tGIh",
    "outputId": "482ca7b9-0a37-4b3f-8168-76628b53b56f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:03<00:00, 1775.75it/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_score_1 = []\n",
    "bleu_score_2 = []\n",
    "bleu_score_3 = []\n",
    "bleu_score_4 = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(fr_preds))):\n",
    "\n",
    "    pred = fr_preds[i].replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").replace(\"<UNK>\", \"\").replace(\"<GO>\", \"\").rstrip()\n",
    "    reference = references[i].lower()\n",
    "#     计算BLEU分数\n",
    "#     score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
    "#     score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0, 1, 0, 0))\n",
    "#     score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 1, 0))\n",
    "#     score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 0, 1))\n",
    "    \n",
    "    \n",
    "    score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
    "    score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0.5, 0.5, 0, 0))\n",
    "    score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0.34, 0.33, 0.33, 0.))\n",
    "    score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    \n",
    "    \n",
    "    bleu_score_1.append(score_1)\n",
    "    bleu_score_2.append(score_2)\n",
    "    bleu_score_3.append(score_3)\n",
    "    bleu_score_4.append(score_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1KcfzTYtGIk",
    "outputId": "38d0bf04-150e-4638-e9a9-b775b2b768aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score cumulative 1-gram on our corpus is about 0.25633715409574215\n",
      "The BLEU score cumulative 2-gram on our corpus is about 0.28271979835871736\n",
      "The BLEU score cumulative 3-gram on our corpus is about 0.3098188594645035\n",
      "The BLEU score cumulative 4-gram on our corpus is about 0.33929627059865847\n"
     ]
    }
   ],
   "source": [
    "print(\"The BLEU score cumulative 1-gram on our corpus is about {}\".format(sum(bleu_score_1) / len(bleu_score_1)))\n",
    "print(\"The BLEU score cumulative 2-gram on our corpus is about {}\".format(sum(bleu_score_2) / len(bleu_score_2)))\n",
    "print(\"The BLEU score cumulative 3-gram on our corpus is about {}\".format(sum(bleu_score_3) / len(bleu_score_3)))\n",
    "print(\"The BLEU score cumulative 4-gram on our corpus is about {}\".format(sum(bleu_score_4) / len(bleu_score_4)))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " Attention_BLEU_SCORE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
