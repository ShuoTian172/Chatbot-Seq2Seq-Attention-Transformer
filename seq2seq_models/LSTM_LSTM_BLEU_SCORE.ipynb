{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LSTM_LSTM_BLEU_SCORE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ARKlQR_6tM2k",
        "m82HZJLctM26"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Ut7JPwZotM1h",
        "colab_type": "code",
        "outputId": "afcad34e-626e-4812-d3db-1f547219a653",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20\n",
        "\n",
        "import os\n",
        "# print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-373541da4631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "LrsUSs66tM1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "lines = open('../input/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('../input/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3983a4469fd5ed2f4b8db5a6cc41eda070c9142f",
        "id": "30IYUs5ctM1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary to map each line's id with its text\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "260fe85783607a5f8190796b4415dee00cdeabc5",
        "id": "c74PSwWxtM1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list of all of the conversations' lines' ids.\n",
        "convs = []\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c7ac8d38b75b1101165c37cdab3fca42597eb4d7",
        "id": "5RsH6otDtM11",
        "colab_type": "code",
        "outputId": "0ed980e0-3779-4a97-c0c7-700f27b4fd1d",
        "colab": {}
      },
      "source": [
        "#id and conversation sample\n",
        "for k in convs[300]:\n",
        "    print (k, id2line[k])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L3490 That's what he did to me.  He put cigarettes out on me.\n",
            "L3491 Your father put cigarettes out on you?\n",
            "L3492 Out on my back when I was a small boy.\n",
            "L3493 Can I see your back?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a3142e705c67fd80837a998a7884dd584c30f57c",
        "id": "AimqCv-ktM16",
        "colab_type": "code",
        "outputId": "089dd19c-eaff-4fde-a3e3-67595e8bdf85",
        "colab": {}
      },
      "source": [
        "# Sort the sentences into questions (inputs) and answers (targets)\n",
        "questions = []\n",
        "answers = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n",
        "        \n",
        "# Compare lengths of questions and answers\n",
        "print(len(questions))\n",
        "print(len(answers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "221616\n",
            "221616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "85f3e1713f0620f066f5a5ee34b31c9a73d768fa",
        "id": "Usb7JdeEtM2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
        "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "162b9f09f2e3e50ed92f157a5ce0faf3bb0d0019",
        "id": "han2vO8-tM2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean the data\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "clean_answers = []    \n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b1df72b880bbf79a8043ae3b608d37e89b5807e9",
        "id": "_WQKGf3VtM2L",
        "colab_type": "code",
        "outputId": "ac184dc6-478c-41e1-df6d-ef7b34892a42",
        "colab": {}
      },
      "source": [
        "# Find the length of sentences (not using nltk due to processing speed)\n",
        "lengths = []\n",
        "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n",
            "19.0\n",
            "24.0\n",
            "32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0035c49ba3da36e903a600b22c737e6a5c34de6e",
        "id": "wH6FUI3JtM2Q",
        "colab_type": "code",
        "outputId": "628f26fc-8346-4b9a-c18a-79c9e4e5cd33",
        "colab": {}
      },
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138528\n",
            "138528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "85389732fd564d7b160339bb26d0d1b5dea41fe7",
        "id": "OmJEk_1ytM2W",
        "colab_type": "code",
        "outputId": "ca7654cd-5618-4757-daa0-c15c33ae1e15",
        "colab": {}
      },
      "source": [
        "r = np.random.randint(1,len(short_questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you asshole! i cannot believe what a !\n",
            "vivian, what the\n",
            "\n",
            "can we talk about this? can you just try to calm down?\n",
            "your goddamned friend, he thinks the only reason i am with you is for the money.\n",
            "\n",
            "fuck off.\n",
            "i will call you a cab. if you want the clothes, pack them up.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5825a5967e5a04f13b0a755e8ba304c419af4b33",
        "id": "HCeHvZMStM2Z",
        "colab_type": "text"
      },
      "source": [
        "### 1.1  Preprocessing for word based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "106248b9b0ac7499c2b34cc3ae1c20a2e0ea41c4",
        "id": "9Pn3gtB5tM2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#choosing number of samples\n",
        "num_samples = 30000  # Number of samples to train on.\n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n",
        "#tokenizing the qns and answers\n",
        "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
        "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8b37316d3b68626ed388bbb73863364608027796",
        "id": "6kXc1b_wtM2c",
        "colab_type": "code",
        "outputId": "acc18ceb-9a11-4f20-f34f-2a9e7d21e996",
        "colab": {}
      },
      "source": [
        "#train-validation split\n",
        "data_size = len(short_questions_tok)\n",
        "\n",
        "# We will use the first 0-80th %-tile (80%) of data for the training\n",
        "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
        "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
        "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
        "\n",
        "# We will use the remaining for validation\n",
        "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
        "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
        "\n",
        "print('training size', len(training_input))\n",
        "print('validation size', len(validation_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training size 24000\n",
            "validation size 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a3aacc0315837faf77599da087f8b3a52d217859",
        "id": "ARKlQR_6tM2k",
        "colab_type": "text"
      },
      "source": [
        "### 1.2  Word en/decoding dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "51918567f4f43ae13a8d58a00bda15b1d508b191",
        "id": "bgFRlPuGtM2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary for the frequency of the vocabulary\n",
        "# Create \n",
        "vocab = {}\n",
        "for question in short_questions_tok:\n",
        "    for word in question:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "for answer in short_answers_tok:\n",
        "    for word in answer:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "30d25056e8f62453bcde0a16c8a7933275f431c9",
        "id": "kyzp9aRYtM2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove rare words from the vocabulary.\n",
        "# We will aim to replace fewer than 5% of words with <UNK>\n",
        "# You will see this ratio soon.\n",
        "threshold = 15\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4c980b035173d12d2bf06dafd6410eb5da6023b2",
        "id": "lEJxn4H-tM2r",
        "colab_type": "code",
        "outputId": "3db30bea-95bc-44c7-a16c-e23455ace108",
        "colab": {}
      },
      "source": [
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 16560\n",
            "Size of vocab we will use: 1938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d62a629773e34bb7bc1a12508cce820c0e47962d",
        "id": "spI13AiPtM2v",
        "colab_type": "code",
        "outputId": "c902b8ed-c8b7-445d-d16e-1271e3d1fa51",
        "colab": {}
      },
      "source": [
        "#we will create dictionaries to provide a unique integer for each word.\n",
        "WORD_CODE_START = 1\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "\n",
        "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
        "encoding = {}\n",
        "decoding = {1: 'START'}\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold: #get vocabularies that appear above threshold count\n",
        "        encoding[word] = word_num \n",
        "        decoding[word_num ] = word\n",
        "        word_num += 1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of vocab used: 1940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "de2e46d3826def6b6c94a9827c32118d555fae2e",
        "id": "EVZQNlA5tM20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#include unknown token for words not in dictionary\n",
        "decoding[len(encoding)+2] = '<UNK>'\n",
        "encoding['<UNK>'] = len(encoding)+2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bccd51e8730e7c3da1234af9480da767f400c230",
        "id": "n__Yf_5gtM23",
        "colab_type": "code",
        "outputId": "fc83fe1b-c1fa-4b3b-9a15-15a76f51aa92",
        "colab": {}
      },
      "source": [
        "dict_size = word_num+1\n",
        "dict_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7ea3c32a6180ee8fea8b1490b40351897cc426e6",
        "id": "m82HZJLctM26",
        "colab_type": "text"
      },
      "source": [
        "### 1.3  Vectorizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4de33f91476bd6c50cf7c9322f273079783ceec2",
        "id": "8zEEW-vAtM27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(encoding, data, vector_size=20):\n",
        "   \n",
        "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "    for i in range(len(data)):\n",
        "        for j in range(min(len(data[i]), vector_size)):\n",
        "            try:\n",
        "                transformed_data[i][j] = encoding[data[i][j]]\n",
        "            except:\n",
        "                transformed_data[i][j] = encoding['<UNK>']\n",
        "    return transformed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f13007fbe8661c4c2ea4c35fd51b86c4fc220efb",
        "id": "8T13CrQZtM29",
        "colab_type": "code",
        "outputId": "e15839b5-bd46-4b1f-e661-c54aac7e3896",
        "colab": {}
      },
      "source": [
        "#encoding training set\n",
        "encoded_training_input = transform(\n",
        "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
        "encoded_training_output = transform(\n",
        "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_training_input', encoded_training_input.shape)\n",
        "print('encoded_training_output', encoded_training_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_training_input (24000, 20)\n",
            "encoded_training_output (24000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cbbf370a5d0640a4dea6b6d9237a5917978b92a8",
        "id": "Bx20p9g7tM2_",
        "colab_type": "code",
        "outputId": "a1012783-c5c5-4029-b1af-c5ebde600958",
        "colab": {}
      },
      "source": [
        "#encoding validation set\n",
        "encoded_validation_input = transform(\n",
        "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "encoded_validation_output = transform(\n",
        "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_validation_input', encoded_validation_input.shape)\n",
        "print('encoded_validation_output', encoded_validation_output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_validation_input (6000, 20)\n",
            "encoded_validation_output (6000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b52f0368fecf8ac79903927799fe67c104d8e518",
        "id": "1rZr5lastM3D",
        "colab_type": "text"
      },
      "source": [
        "## 2  Model Building\n",
        "### 2.1  Sequence-to-Sequence in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e8b5afc77468e4e263b3fc7ff62a2e6952a80e97",
        "id": "4khr-xRPtM3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "306b1fb012febd74dd40840c0588bdd8503ba65b",
        "id": "byk7jb3-tM3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20\n",
        "\n",
        "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
        "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP-s2CZdtM3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder Setup\n",
        "\n",
        "enc_emb_look_up = Embedding(dict_size, 512, input_length=INPUT_LENGTH, mask_zero=True)\n",
        "\n",
        "enc_emb_text = enc_emb_look_up(encoder_input)\n",
        "\n",
        "encoder_lstm = LSTM(512, return_state=True, name='encoder_lstm', dropout=0.2) # To return the final state of the encoder\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb_text)\n",
        "\n",
        "encoder_states = [state_h, state_c] # Discard encoder_outputs (at each time step) and only keep the final states.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d91367ec6c706059c4b520011fdec6e1051db38d",
        "id": "lZFZAKMStM3M",
        "colab_type": "code",
        "outputId": "72328e49-b0d4-40a8-8d1a-aedf2a7ea09c",
        "colab": {}
      },
      "source": [
        "from keras.layers import SimpleRNN\n",
        "\n",
        "\n",
        "decoder = Embedding(dict_size, 512, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
        "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=encoder_states)\n",
        "\n",
        "print('decoder', decoder)\n",
        "\n",
        "\n",
        "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(decoder)\n",
        "print('output', output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
            "output Tensor(\"time_distributed_1/Reshape_1:0\", shape=(?, 20, 1941), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "257dcf12770f8928f91754044d3efc5aff1ec296",
        "id": "-pWu6816tM3U",
        "colab_type": "code",
        "outputId": "c3ec0fe0-846e-4477-80fa-1147b69539c9",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 512)      993792      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 512)      993792      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm (LSTM)             [(None, 512), (None, 2099200     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 512)      2099200     embedding_2[0][0]                \n",
            "                                                                 encoder_lstm[0][1]               \n",
            "                                                                 encoder_lstm[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 1941)     995733      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 7,181,717\n",
            "Trainable params: 7,181,717\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "09b61276a10c14838eb09db124a229ac58c61133",
        "id": "0xFjW7TktM3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_encoder_input = encoded_training_input\n",
        "training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
        "training_decoder_input[:, 0] = WORD_CODE_START\n",
        "training_decoder_output =np.eye(dict_size)[encoded_training_output.astype('int')]\n",
        "\n",
        "validation_encoder_input = encoded_validation_input\n",
        "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
        "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
        "validation_decoder_input[:, 0] = WORD_CODE_START\n",
        "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3d43e174878dfbef64900b7f043d7bb41f458309",
        "id": "b5K5MS8KtM3k",
        "colab_type": "code",
        "outputId": "abe47222-0cc5-4ff3-908d-31613e263425",
        "colab": {}
      },
      "source": [
        "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
        "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
        "          #validation_split=0.05,\n",
        "          batch_size=64, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 24000 samples, validate on 6000 samples\n",
            "Epoch 1/50\n",
            "24000/24000 [==============================] - 39s 2ms/step - loss: 0.0032 - val_loss: 0.0028\n",
            "Epoch 2/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 3/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 4/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 5/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 6/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 7/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 8/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 9/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 10/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 11/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 12/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 13/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 14/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 15/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 16/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 17/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 18/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 19/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 20/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 21/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 22/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 23/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 24/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0019 - val_loss: 0.0023\n",
            "Epoch 25/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 26/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 27/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 28/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 29/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 30/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0017 - val_loss: 0.0024\n",
            "Epoch 31/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 32/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 33/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 34/50\n",
            "24000/24000 [==============================] - 30s 1ms/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 35/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 36/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 37/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 38/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 39/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 40/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 41/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 42/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 43/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 44/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 45/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 46/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 47/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 48/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 49/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 50/50\n",
            "24000/24000 [==============================] - 29s 1ms/step - loss: 0.0010 - val_loss: 0.0031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9dec74630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "874f0398f10f48cce993bcd9aa7b5e0644b6f292",
        "collapsed": true,
        "id": "kSqka1witM3q",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d0e3217f3d250c253dd457f192f54f8cd67c630f",
        "id": "c2Px9rZBtM3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(raw_input):\n",
        "    clean_input = clean_text(raw_input)\n",
        "    input_tok = [nltk.word_tokenize(clean_input)]\n",
        "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
        "    encoder_input = transform(encoding, input_tok, 20)\n",
        "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "    decoder_input[:,0] = WORD_CODE_START\n",
        "    for i in range(1, OUTPUT_LENGTH):\n",
        "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
        "        decoder_input[:,i] = output[:,i]\n",
        "    return output\n",
        "\n",
        "def decode(decoding, vector):\n",
        "    \n",
        "    text = ''\n",
        "    for i in vector:\n",
        "        if i == 0:\n",
        "            break\n",
        "        text += ' '\n",
        "        text += decoding[i]\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "16e6ece57e4c02321c58176e5471f37ad1170837",
        "id": "dX4q7osstM3t",
        "colab_type": "code",
        "outputId": "c7916e31-59e1-4bb4-dcf1-bbd751a7bed6",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "    seq_index = np.random.randint(1, len(short_questions))\n",
        "    output = prediction(short_questions[seq_index])\n",
        "    print ('Q:', short_questions[seq_index])\n",
        "    print ('A:', decode(decoding, output[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: because marrying that fool king westley is\n",
            "A:  i am <UNK> a doctor .\n",
            "Q: presently, mr. preysing.\n",
            "A:  you are <UNK> with your crew ?\n",
            "Q: so that is the one you want to marry.\n",
            "A:  no . i am not .\n",
            "Q: let's talk about it some other time.\n",
            "A:  what are you doing here ?\n",
            "Q: and outpost? you are happy there?\n",
            "A:  no . i was just a little woman .\n",
            "Q: you live here?\n",
            "A:  no . i was not the <UNK> .\n",
            "Q: you do, you do. you are just not saying.\n",
            "A:  no , i do not . i mean that you are not about to find her or something 's 's\n",
            "Q: a unicorngs been slain. the last stallion in all the country.\n",
            "A:  we are not going to <UNK> .\n",
            "Q: you know her?\n",
            "A:  you are right .\n",
            "Q: i lied. the journey will be longer than i said.\n",
            "A:  i will give you a <UNK> for me .\n",
            "Q: i am father vogler. i am a chaplain here. i thought you might like to talk to someone.\n",
            "A:  i know . i am sorry .\n",
            "Q: joyce can be nice.\n",
            "A:  you are coming .\n",
            "Q: he is here.\n",
            "A:  he is <UNK> . he was alive .\n",
            "Q: they are coming!\n",
            "A:  they are <UNK> us , mollie .\n",
            "Q: this is my business manager, rowboat.\n",
            "A:  <UNK> is not a good one .\n",
            "Q: i understand that if i had a few more friends like you and bob i would be dead.\n",
            "A:  i will give you a <UNK> .\n",
            "Q: no. i am sure i have never met anyone of such a bizarre appearance.\n",
            "A:  i am <UNK> the <UNK> . i can not have this <UNK> .\n",
            "Q: hey, do you have any glue in your bag?\n",
            "A:  no . i am not . why ?\n",
            "Q: you are the one who's still living out some stupid seventhgrade fantasy!\n",
            "A:  you are <UNK> ?\n",
            "Q: two months shy of four years.\n",
            "A:  i will be right back .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0d5742477af42914841bb341532f7e9e01c15815",
        "id": "_oTvuZeAtM3v",
        "colab_type": "code",
        "outputId": "ffa04dca-c078-480f-a9da-4440a387fa79",
        "colab": {}
      },
      "source": [
        "raw_input = input()\n",
        "output = prediction(raw_input)\n",
        "print (decode(decoding, output[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            " yeah , i am <UNK> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTV-wx2otM3z",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3wklhNYtM32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uwkEtShtM38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = short_questions\n",
        "Y = short_answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PktdJL6ptM4A",
        "colab_type": "code",
        "outputId": "7d49d6a4-8040-40ac-aeb8-72cf64e7e646",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "len (Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E9Ga7nDtM4I",
        "colab_type": "code",
        "outputId": "b1caed2a-302a-405f-e6ec-bd3225959bac",
        "colab": {}
      },
      "source": [
        "\n",
        "fr_preds = []\n",
        "\n",
        "for sentence in tqdm.tqdm(X_test):\n",
        "    fr_pred = decode(decoding, prediction(sentence)[0])\n",
        "    \n",
        "\n",
        "    fr_preds.append(fr_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [35:16<00:00,  2.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmW3phC3tM4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "references = Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QblljCrUtM4K",
        "colab_type": "code",
        "outputId": "08cc1925-13a0-466f-896c-aa62756c7874",
        "colab": {}
      },
      "source": [
        "\n",
        "bleu_score_1 = []\n",
        "bleu_score_2 = []\n",
        "bleu_score_3 = []\n",
        "bleu_score_4 = []\n",
        "\n",
        "for i in tqdm.tqdm(range(len(fr_preds))):\n",
        "\n",
        "    pred = fr_preds[i].replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").replace(\"<UNK>\", \"\").replace(\"<GO>\", \"\").rstrip()\n",
        "    reference = references[i].lower().replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").replace(\"<UNK>\", \"\").replace(\"<GO>\", \"\").rstrip()\n",
        "\n",
        "    score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
        "    score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0, 1, 0, 0))\n",
        "    score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 1, 0))\n",
        "    score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 0, 1))\n",
        "    \n",
        "    \n",
        "#     score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
        "#     score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0.5, 0.5, 0, 0))\n",
        "#     score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0.34, 0.33, 0.33, 0.))\n",
        "#     score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    \n",
        "    \n",
        "    \n",
        "    bleu_score_1.append(score_1)\n",
        "    bleu_score_2.append(score_2)\n",
        "    bleu_score_3.append(score_3)\n",
        "    bleu_score_4.append(score_4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6000 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 6000/6000 [00:03<00:00, 1981.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDd1zS8_tM4M",
        "colab_type": "code",
        "outputId": "d15dd0fe-af46-4553-be81-3344a9efddd4",
        "colab": {}
      },
      "source": [
        "print(\"The BLEU score individual 1-gram on our corpus is about {}\".format(sum(bleu_score_1) / len(bleu_score_1)))\n",
        "print(\"The BLEU score individual 2-gram on our corpus is about {}\".format(sum(bleu_score_2) / len(bleu_score_2)))\n",
        "print(\"The BLEU score individual 3-gram on our corpus is about {}\".format(sum(bleu_score_3) / len(bleu_score_3)))\n",
        "print(\"The BLEU score individual 4-gram on our corpus is about {}\".format(sum(bleu_score_4) / len(bleu_score_4)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BLEU score individual 1-gram on our corpus is about 0.07338952550660324\n",
            "The BLEU score individual 2-gram on our corpus is about 0.2907444558916987\n",
            "The BLEU score individual 3-gram on our corpus is about 0.3318095268709505\n",
            "The BLEU score individual 4-gram on our corpus is about 0.343996769189953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPAY6BFDtM4O",
        "colab_type": "code",
        "outputId": "17bcb157-60d4-4a59-8518-ba2387781ff7",
        "colab": {}
      },
      "source": [
        "\n",
        "bleu_score_1 = []\n",
        "bleu_score_2 = []\n",
        "bleu_score_3 = []\n",
        "bleu_score_4 = []\n",
        "\n",
        "for i in tqdm.tqdm(range(len(fr_preds))):\n",
        "\n",
        "    pred = fr_preds[i].replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").replace(\"<UNK>\", \"\").replace(\"<GO>\", \"\").rstrip()\n",
        "    reference = references[i].lower()\n",
        "\n",
        "#     score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
        "#     score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0, 1, 0, 0))\n",
        "#     score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 1, 0))\n",
        "#     score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0, 0, 0, 1))\n",
        "    \n",
        "    \n",
        "    score_1 = sentence_bleu([reference.split()], pred.split(), weights=(1, 0, 0, 0))\n",
        "    score_2 = sentence_bleu([reference.split()], pred.split(), weights=(0.5, 0.5, 0, 0))\n",
        "    score_3 = sentence_bleu([reference.split()], pred.split(), weights=(0.34, 0.33, 0.33, 0.))\n",
        "    score_4 = sentence_bleu([reference.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    \n",
        "    \n",
        "    \n",
        "    bleu_score_1.append(score_1)\n",
        "    bleu_score_2.append(score_2)\n",
        "    bleu_score_3.append(score_3)\n",
        "    bleu_score_4.append(score_4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:03<00:00, 1987.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiKl-onytM4Q",
        "colab_type": "code",
        "outputId": "361cf284-f7d6-45b2-ecac-dedee9da49d0",
        "colab": {}
      },
      "source": [
        "print(\"The BLEU score cumulative 1-gram on our corpus is about {}\".format(sum(bleu_score_1) / len(bleu_score_1)))\n",
        "print(\"The BLEU score cumulative 2-gram on our corpus is about {}\".format(sum(bleu_score_2) / len(bleu_score_2)))\n",
        "print(\"The BLEU score cumulative 3-gram on our corpus is about {}\".format(sum(bleu_score_3) / len(bleu_score_3)))\n",
        "print(\"The BLEU score cumulative 4-gram on our corpus is about {}\".format(sum(bleu_score_4) / len(bleu_score_4)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BLEU score cumulative 1-gram on our corpus is about 0.07338952550660324\n",
            "The BLEU score cumulative 2-gram on our corpus is about 0.13019317920387147\n",
            "The BLEU score cumulative 3-gram on our corpus is about 0.17214531520709955\n",
            "The BLEU score cumulative 4-gram on our corpus is about 0.20463749715742582\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}