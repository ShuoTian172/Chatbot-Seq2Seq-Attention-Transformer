{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "XIjfApJ12Zrl"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "d=300\n",
    "INPUT_LENGTH = d\n",
    "OUTPUT_LENGTH = d\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "pW7_0rWC2Zrq"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('../input/cornell-moviedialog-corpus/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('../input/cornell-moviedialog-corpus/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "3983a4469fd5ed2f4b8db5a6cc41eda070c9142f",
    "colab": {},
    "colab_type": "code",
    "id": "30GAyJEm2Zrs"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "260fe85783607a5f8190796b4415dee00cdeabc5",
    "colab": {},
    "colab_type": "code",
    "id": "499zhQQq2Zrv"
   },
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c7ac8d38b75b1101165c37cdab3fca42597eb4d7",
    "colab": {},
    "colab_type": "code",
    "id": "0ga3DyOS2Zrx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "a3142e705c67fd80837a998a7884dd584c30f57c",
    "colab": {},
    "colab_type": "code",
    "id": "KuKSZJEN2Zrz"
   },
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "# Compare lengths of questions and answers\n",
    "\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "85f3e1713f0620f066f5a5ee34b31c9a73d768fa",
    "colab": {},
    "colab_type": "code",
    "id": "URfYnXIr2Zr2"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "162b9f09f2e3e50ed92f157a5ce0faf3bb0d0019",
    "colab": {},
    "colab_type": "code",
    "id": "EmAFWsHJ2Zr4"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "0035c49ba3da36e903a600b22c737e6a5c34de6e",
    "colab": {},
    "colab_type": "code",
    "id": "xuy4yBLA2Zr7"
   },
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "for i, question in enumerate(clean_questions):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for i, answer in enumerate(short_answers_temp):\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "        \n",
    "print(len(short_questions))\n",
    "print(len(short_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "85389732fd564d7b160339bb26d0d1b5dea41fe7",
    "colab": {},
    "colab_type": "code",
    "id": "qYY5QgW-2Zr-"
   },
   "outputs": [],
   "source": [
    "del convs\n",
    "del id2line \n",
    "del clean_questions\n",
    "del clean_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5825a5967e5a04f13b0a755e8ba304c419af4b33",
    "colab_type": "text",
    "id": "IvLeDknb2ZsB"
   },
   "source": [
    "### 1.1  Preprocessing for word based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "106248b9b0ac7499c2b34cc3ae1c20a2e0ea41c4",
    "colab": {},
    "colab_type": "code",
    "id": "wMWGciz42ZsC"
   },
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 1500  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "8b37316d3b68626ed388bbb73863364608027796",
    "colab": {},
    "colab_type": "code",
    "id": "6vQY8pDm2ZsF"
   },
   "outputs": [],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3aacc0315837faf77599da087f8b3a52d217859",
    "colab_type": "text",
    "id": "_CcIUy922ZsH"
   },
   "source": [
    "### 1.2  Word en/decoding dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "51918567f4f43ae13a8d58a00bda15b1d508b191",
    "colab": {},
    "colab_type": "code",
    "id": "pDvw3Skp2ZsK"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "# Create \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "30d25056e8f62453bcde0a16c8a7933275f431c9",
    "colab": {},
    "colab_type": "code",
    "id": "Sm_JXEOv2ZsN"
   },
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 5\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4c980b035173d12d2bf06dafd6410eb5da6023b2",
    "colab": {},
    "colab_type": "code",
    "id": "kI8CYJXD2ZsP"
   },
   "outputs": [],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d62a629773e34bb7bc1a12508cce820c0e47962d",
    "colab": {},
    "colab_type": "code",
    "id": "b7zu8ouI2ZsR"
   },
   "outputs": [],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "de2e46d3826def6b6c94a9827c32118d555fae2e",
    "colab": {},
    "colab_type": "code",
    "id": "AnyP_cCJ2ZsU"
   },
   "outputs": [],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "bccd51e8730e7c3da1234af9480da767f400c230",
    "colab": {},
    "colab_type": "code",
    "id": "zbB4pQaT2ZsW"
   },
   "outputs": [],
   "source": [
    "dict_size = word_num+1\n",
    "dict_size\n",
    "def transform(encoding, data, vector_size):\n",
    "   \n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ea3c32a6180ee8fea8b1490b40351897cc426e6",
    "colab_type": "text",
    "id": "2lIbEIF32ZsY"
   },
   "source": [
    "### 1.3  Vectorizing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GloVe \n",
    "# embeddings = {}\n",
    "# f = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n",
    "# #chang the path to different dimension glove pre-trained model\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     vector = np.asarray(values[1:], dtype='float32')\n",
    "#     #print(vector)\n",
    "#     embeddings[word] = vector\n",
    "# f.close()\n",
    "# #print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpDm_o2c2ZsZ"
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "f = '../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(f,\n",
    "        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4de33f91476bd6c50cf7c9322f273079783ceec2",
    "colab": {},
    "colab_type": "code",
    "id": "K7BjHq2U2Zsc"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder_embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(encoding)+1, d)) \n",
    "\n",
    "for word, i in encoding.items(): # i=0 is the embedding for the zero padding\n",
    "    if word in word2vec.vocab:\n",
    "        encoder_embeddings_matrix[i] = word2vec.word_vec(word)\n",
    "#print(encoder_embeddings_matrix)\n",
    "\n",
    "decoder_embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(decoding)+1, d)) \n",
    "\n",
    "for word, i in decoding.items(): # i=0 is the embedding for the zero padding\n",
    "    if word in word2vec.vocab:\n",
    "        decoder_embeddings_matrix[i] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b52f0368fecf8ac79903927799fe67c104d8e518",
    "colab_type": "text",
    "id": "XM4UQnS12Zsg"
   },
   "source": [
    "## 2  Model Building\n",
    "### 2.1  Sequence-to-Sequence in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "e8b5afc77468e4e263b3fc7ff62a2e6952a80e97",
    "colab": {},
    "colab_type": "code",
    "id": "X2Qt-umc2Zsh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "306b1fb012febd74dd40840c0588bdd8503ba65b",
    "colab": {},
    "colab_type": "code",
    "id": "3UhCslxv2Zsj"
   },
   "outputs": [],
   "source": [
    "INPUT_LENGTH = d\n",
    "OUTPUT_LENGTH = d\n",
    "\n",
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d91367ec6c706059c4b520011fdec6e1051db38d",
    "colab": {},
    "colab_type": "code",
    "id": "KNjrbsfJ2Zsl"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "encoder = Embedding(dict_size-1, d, input_length=INPUT_LENGTH, weights = [encoder_embeddings_matrix], mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)\n",
    "\n",
    "decoder = Embedding(dict_size, d, input_length=OUTPUT_LENGTH,weights = [decoder_embeddings_matrix], mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "print('decoder', decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2feb58fa60a8ff335b218114808aa2d3e4d37461",
    "colab_type": "text",
    "id": "43zWMVbo2Zsp"
   },
   "source": [
    "### 2.2  Attention Mechanism\n",
    "Reference: Effective Approaches to Attention-based Neural Machine Translation's Global Attention with Dot-based scoring function (Section 3, 3.1) https://arxiv.org/pdf/1508.04025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBa_Uu1R2Zsp"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('attention', attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "print('context', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "257dcf12770f8928f91754044d3efc5aff1ec296",
    "colab": {},
    "colab_type": "code",
    "id": "roM5WN_l2Zss"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "09b61276a10c14838eb09db124a229ac58c61133",
    "colab": {},
    "colab_type": "code",
    "id": "cgT1NIvI2Zsu"
   },
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_inputtraining_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output =np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START \n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "3d43e174878dfbef64900b7f043d7bb41f458309",
    "colab": {},
    "colab_type": "code",
    "id": "dGZb3yjb2Zsw"
   },
   "outputs": [],
   "source": [
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          #validation_split=0.05,\n",
    "          batch_size=64, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "874f0398f10f48cce993bcd9aa7b5e0644b6f292",
    "colab_type": "text",
    "collapsed": true,
    "id": "wJgGw_4J2Zsy"
   },
   "source": [
    "## 3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d0e3217f3d250c253dd457f192f54f8cd67c630f",
    "colab": {},
    "colab_type": "code",
    "id": "pvctQbzz2Zsz"
   },
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    clean_input = clean_text(raw_input)\n",
    "    input_tok = [nltk.word_tokenize(clean_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, d)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output\n",
    "\n",
    "def decode(decoding, vector):\n",
    "    \n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "16e6ece57e4c02321c58176e5471f37ad1170837",
    "colab": {},
    "colab_type": "code",
    "id": "CyYLukY42Zs2"
   },
   "outputs": [],
   "source": [
    "test_que=[]\n",
    "test_ans=[]\n",
    "test_r=[]\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "for i in range(100):\n",
    "    output = prediction(TreebankWordDetokenizer().detokenize(validation_input[i]))\n",
    "    q=validation_input[i]\n",
    "    a=decode(decoding, output[0])\n",
    "    r=validation_output[i]\n",
    "    test_que.append(q)\n",
    "    test_ans.append(a)\n",
    "    test_r.append(r)\n",
    "tok_r=[]\n",
    "tok_ans = [nltk.word_tokenize(sent) for sent in test_ans]\n",
    "for i in range (0,len(test_r)):\n",
    "    p=test_r[i]\n",
    "    tok_r.append([p])\n",
    "#print(tok_ans[:9])\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "reference = tok_r\n",
    "candidate = tok_ans\n",
    "print('Cumulative 1-gram: %f' % corpus_bleu(reference, candidate))\n",
    "print('Cumulative 2-gram: %f' % corpus_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % corpus_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % corpus_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "0d5742477af42914841bb341532f7e9e01c15815",
    "colab": {},
    "colab_type": "code",
    "id": "dyq1IZy_2Zs4"
   },
   "outputs": [],
   "source": [
    "raw_input = input()\n",
    "output = prediction(raw_input)\n",
    "print (decode(decoding, output[0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "chatbot_w2v_300d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
